# Program Repair with LLMs: Text, Graph, and Coverage-Based Analysis

## Overview

This project explores automated software bug detection and program repair using Large Language Models (LLMs). We compare two main input modalities:

1. **Plain Code Text** â€” Providing source code directly to LLMs.
2. **Graph-Enhanced Representation** â€” Feeding LLMs with a combination of:
   - Code property graphs (CPG), abstract syntax trees (AST), or other graph structures derived from code.
   - **Test coverage information** (e.g., which parts of the code are executed by tests).
   - **Log and bug report data** associated with code or its graph.

Our aim is to evaluate if and how these richer, multi-modal graph-based representations improve bug localization and automated repair capabilities of LLMs compared to plain code text alone.

---

## Directory Structure

```markdown
program-repair-llm/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ bug_reports/        # Extracted or original bug reports and logs
â”‚   â”œâ”€â”€ coverage/           # Code test coverage reports/matrices
â”‚   â”œâ”€â”€ graphs/             # Graph representations: ASTs, CPGs, etc.
â”‚   â”œâ”€â”€ processed/          # Preprocessed/cleaned datasets for experiments
â”‚   â””â”€â”€ raw/                # Original datasets and sources (e.g. CodRep)
â”œâ”€â”€ models/                 # Model checkpoints, pretrained or finetuned
â”œâ”€â”€ notebooks/              # Jupyter notebooks for EDA, prototyping, experiments
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ logs/               # Experiment logs and outputs
â”‚   â””â”€â”€ reports/            # Generated reports and result summaries
â”œâ”€â”€ scripts/                # Data processing and analysis scripts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ bug_report_extract.py
â”‚   â”œâ”€â”€ code_to_graph.py
â”‚   â”œâ”€â”€ coverage_analysis.py
â”‚   â””â”€â”€ generate_faulty_code.py
â”œâ”€â”€ src/                    # Core logic and experiment modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ llm_graph_input.py
â”‚   â”œâ”€â”€ llm_text_input.py
â”‚   â”œâ”€â”€ metrics.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

## Requirements

- Python 3.8 or higher

- All Python dependencies specified in requirements.txt:

```bash
pip install -r requirements.txt
```

- GitHub CLI (gh) for automatic repository downloading and cloning:

- Install from the [GitHub CLI installation guide](https://docs.github.com/en/github-cli/github-cli/quickstart).

- After installation, authenticate using:

```bash
gh auth login
```

## ðŸ§  Running LLMs Locally with Ollama (Windows)

We use [Ollama](https://ollama.com) to run models like **Mistral** and **DeepSeek-Coder** locally.

### âœ… Requirements

- Windows 10/11 (x86_64)
- At least 16GB RAM
- A GPU with 8GB+ VRAM (e.g. RTX 3060 Ti recommended)
- WSL not required

### ðŸ”§ Step 1: Install Ollama

Download the installer from the official website:

ðŸ‘‰ [https://ollama.com](https://ollama.com)

Click **Download for Windows**, then run the `.exe` installer.

After installation, the Ollama service starts automatically and listens at:

```
http://localhost:11434
```

You can test it by running the following in PowerShell or Command Prompt:

```powershell
ollama --help
```

---

### ðŸ“¥ Step 2: Pull and Run Models

#### Mistral

```powershell
ollama pull mistral
ollama run mistral
```

#### DeepSeek-Coder

```powershell
ollama pull deepseek-coder
ollama run deepseek-coder
```

---


## Retrieving issues 

Github CLI was used for retrieving issues. 
After logging in as shown above, clone the directory of the github to retrieve issues from.
Go into the directory and use these commands: 

``` bash
gh issue list --state open --json author,body,comments,createdAt,number,state,title,url --limit 900  >pygithub_issues_open.json
gh issue list --state closed --json author,body,comments,createdAt,number,state,title,url --limit 900  >pygithub_issues_closed.json
```

Note that there are primary and secondary limits on Github API requests. The limiting factor here is the secondary limit of 900 per minute.
To work around this limit if there are more than 900 issues available, a search option can be added to the command with a date range to ensure no more than 900 are requested at a time.
For example: 
``` bash
 gh issue list --state closed --search "created:<=2019-10-24" --json author,body,comments,createdAt,number,state,title,url --limit 900  >pygithub_issues_2_closed.json
```